{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.data_dir = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading VGG16 Model ...\n",
      "- Download progress: 100.0%\n",
      "Download finished. Extracting files.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "vgg16.maybe_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(filename, max_size=None):\n",
    "    image = PIL.Image.open(filename)\n",
    "    \n",
    "    if max_size is not None:\n",
    "        # Rescale factor\n",
    "        factor = max_size / np.max(image.size)\n",
    "        \n",
    "        # Scale the image\n",
    "        size = np.array(image.size) * factor\n",
    "        \n",
    "        # Convert floating point size to int\n",
    "        size = size.astype(int)\n",
    "        \n",
    "        # Resize the image\n",
    "        image = image.resize(size, PIL.Image.LANCZOS)\n",
    "        \n",
    "    # Convert to numpy floating-point array\n",
    "    return np.float32(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(image, filename):\n",
    "    \"\"\"\n",
    "        Save image as jpg\n",
    "    \"\"\"\n",
    "    image = np.clip(image, 0.0, 255.0)\n",
    "    \n",
    "    # Convert to bytes\n",
    "    image = image.astype(np.uint8)\n",
    "    \n",
    "    # Write the image-file in JPG\n",
    "    with open(filename, 'wb') as file:\n",
    "        PIL.Image.fromarray(image).save(file, 'jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_big(image):\n",
    "    \"\"\"\n",
    "        Plots a large image\n",
    "    \"\"\"\n",
    "    image = np.clip(image, 0.0, 255.0)\n",
    "    \n",
    "    image = image.astype(np.uint8)\n",
    "    \n",
    "    display(PIL.Image.fromarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(content_image, style_image, mixed_image):\n",
    "    \"\"\"\n",
    "        Displays the content, style and mixed images together\n",
    "    \"\"\"\n",
    "    # Create figure with sub-plots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 10))\n",
    "    \n",
    "    # Adjust vertical spacing\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    \n",
    "    # Use interpolation to smooth pixels\n",
    "    smooth = True\n",
    "    \n",
    "    # Interpolation type\n",
    "    if smooth:\n",
    "        interpolation = 'sinc'\n",
    "    else:\n",
    "        interpolation = 'nearest'\n",
    "        \n",
    "    # Plot normalized pixel values\n",
    "    # Content-image\n",
    "    ax = axes.flat[0]\n",
    "    ax.imshow(content_image / 255.0, interpolation=interpolation)\n",
    "    ax.set_xlabel(\"Content\")\n",
    "    \n",
    "    # Mixed-image\n",
    "    ax = axes.flat[1]\n",
    "    ax.imshow(mixed_image / 255.0, interpolation=interpolation)\n",
    "    ax.set_xlabel('Mixed')\n",
    "    \n",
    "    # Style-image\n",
    "    ax = axes.flat[2]\n",
    "    ax.imshow(style_image / 255.0, interpolation=interpolation)\n",
    "    ax.set_xlabel('Style')\n",
    "    \n",
    "    # Remove ticks\n",
    "    for ax in axes.flat:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(a, b):\n",
    "    return tf.reduce_mean(tf.square(a - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_content_loss(session, model, content_image, layer_ids):\n",
    "    # Create feed-dict for content-image\n",
    "    feed_dict = model.create_feed_dict(image=content_image)\n",
    "    \n",
    "    # Get references to tensors for given layers\n",
    "    layers = model.get_layer_tensors(layer_ids)\n",
    "    \n",
    "    # Calculate output of those layers for content-image\n",
    "    values = session.run(layers, feed_dict=feed_dict)\n",
    "    \n",
    "    with model.garph.as_default():\n",
    "        layer_losses = []\n",
    "        \n",
    "        for value, layer in zip(values, layers):\n",
    "            value_const = tf.constant(value)\n",
    "            \n",
    "            loss = mean_squared_error(layer, value_const)\n",
    "            \n",
    "            layer_losses.append(loss)\n",
    "        \n",
    "        total_loss = tf.reduce_mean(layer_losses)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    \"\"\"\n",
    "        Finds which features activate simultaneously\n",
    "    \"\"\"\n",
    "    num_channels = int(shape[3])\n",
    "    \n",
    "    matrix = tf.reshape(tensor, shape=[-1, num_channels])\n",
    "    \n",
    "    gram = tf.matmul(tf.transpose(matrix), matrix)\n",
    "    \n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_style_loss(session, model, style_image, layer_ids):\n",
    "    feed_dict = model.create_feed_dict(image=style_image)\n",
    "    \n",
    "    layers = model.get_layer_tensors(layer_ids)\n",
    "    \n",
    "    with model.graph.as_default():\n",
    "        gram_layers = [gram_matrix(layer) for layer in layers]\n",
    "        \n",
    "        values = session.run(gram_layers, feed_dict=feed_dict)\n",
    "        \n",
    "        layer_losses = []\n",
    "        \n",
    "        for value, gram_layer in zip(values, gram_layers):\n",
    "            value_const = tf.constant(value)\n",
    "            \n",
    "            loss = mean_squared_error(gram_layer, value_const)\n",
    "            \n",
    "            layer_losses.append(loss)\n",
    "        \n",
    "        total_loss = tf.reduce_mean(layer_losses)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_denoise_loss(model):\n",
    "    loss = tf.reduce_sum(tf.abs(model.input[:, 1:, :, :] - model.input[:, :-1, :, :])) + \\\n",
    "            tf.reduce_sum(tf.abs(model.input[:, :, 1:, :] - model.input[:, :, :-1, :]))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transfer(content_image, style_image,\n",
    "                  content_layer_ids, style_layer_ids,\n",
    "                  weight_content=1.5, weight_style=10.0,\n",
    "                  weight_denoise=0.3,\n",
    "                  num_iterations=120, step_size=10.0):\n",
    "    # Create instance of VGG model\n",
    "    model = vgg16.VGG16()\n",
    "    \n",
    "    session = tf.InteractiveSession(graph=model.graph)\n",
    "    \n",
    "    print(\"Content layers:\")\n",
    "    print(model.get_layer_names(content_layer_ids))\n",
    "    print()\n",
    "    \n",
    "    print(\"Style layers:\")\n",
    "    print(model.get_layer_names(style_layer_ids))\n",
    "    print()\n",
    "    \n",
    "    loss_content = create_content_loss(session=session, model=model, content_image=content_image, \n",
    "                                       layer_ids=content_layer_ids)\n",
    "    \n",
    "    loss_style = create_style_loss(session=session, model=model, style_image=style_image, \n",
    "                                   layer_ids=content_layer_ids)\n",
    "    \n",
    "    loss_denoise = create_denoise_loss(model)\n",
    "    \n",
    "    adj_content = tf.Variable(1e-10, name='adj_content')\n",
    "    adj_style = tf.Variable(1e-10, name='adj_style')\n",
    "    adj_denoise = tf.Variable(1e-10, name='adj_denoise')\n",
    "    \n",
    "    update_adj_content = adj_content.assign(1.0 / (loss_content + 1e-10))\n",
    "    update_adj_style = adj_style.assign(1.0 / (loss_style + 1e-10))\n",
    "    update_adj_denoise = adj_denoise.assign(1.0 / (loss_denoise + 1e-10))\n",
    "    \n",
    "    loss_combined = weight_content * adj_content * loss_content + \\\n",
    "                    weight_style * adj_style * loss_style + \\\n",
    "                    weight_denoise * adj_denoise * loss_denoise\n",
    "    \n",
    "    gradient = tf.gradients(loss_combined, model.input)\n",
    "    \n",
    "    run_list = [gradient, update_adj_content, update_adj_style, update_adj_denoise]\n",
    "    \n",
    "    mixed_image = np.random.rand(*content_image.shape) + 128\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        feed_dict = model.create_feed_dict(image=mixed_image)\n",
    "        \n",
    "        grad, adj_content_val, adj_style_val, adj_denoise_val = session.run(run_list, feed_dict=feed_dict)\n",
    "        \n",
    "        grad = np.squeeze(grad)\n",
    "        \n",
    "        step_size_scaled = step_size / (np.std(grad) + 1e-8)\n",
    "        \n",
    "        mixed_image -= grad * step_size_scaled\n",
    "        \n",
    "        mixed_image = np.clip(mixed_image, 0.0, 255.0)\n",
    "        \n",
    "        print(\". \", end=\"\")\n",
    "        \n",
    "        if (i % 10 == 0) or (i == num_iterations - 1):\n",
    "            print()\n",
    "            print(\"Iteration:\", i)\n",
    "            \n",
    "            msg = \"Weight Adj. for Content: {0:.2e}, Style: {1:.2e}, Denoise: {2:.2e}\"\n",
    "            print(msg.format(adj_content_val, adj_style_val, adj_denoise_val))\n",
    "            \n",
    "            plot_images(content_image=content_image, style_image=style_image, mixed_image=mixed_image)\n",
    "    print()\n",
    "    print(\"Final image\")\n",
    "    plot_image_big(mixed_image)\n",
    "    \n",
    "    session.close()\n",
    "    \n",
    "    return mixed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_filename = 'willy_wonka_old.jpg'\n",
    "content_image = load_image(content_filename, max_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_filename = \"style2.jpg\"\n",
    "style_image = load_image(style_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layer_ids = [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_layer_ids = list(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "img = style_transfer(content_image=content_image,\n",
    "                    style_image=style_image,\n",
    "                    content_layer_ids=content_layer_ids,\n",
    "                    style_layer_ids=style_layer_ids,\n",
    "                    weight_content=1.5,\n",
    "                    weight_style=10.0,\n",
    "                    weight_denoise=0.3,\n",
    "                    num_iterations=60,\n",
    "                    step_size=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
